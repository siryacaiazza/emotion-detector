{"cells":[{"cell_type":"markdown","source":["# Dashboard for the Emotion Classifier\n","The instruction to launch the dashboard are in the README.md file of my [Github repository](https://github.com/siryacaiazza/emotion-detector)"],"metadata":{"id":"CUrPuDg7ZHIk"}},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":15566,"status":"ok","timestamp":1752341604298,"user":{"displayName":"Sirya Caiazza","userId":"12380628864345214682"},"user_tz":-120},"id":"gsDJW6eRBySe","colab":{"base_uri":"https://localhost:8080/"},"outputId":"aaeb0e4e-e668-4c49-954a-4e16d6442f63"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["# Install required libraries\n","\n","!pip install -q streamlit\n","\n","!pip install -q pyngrok"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1752341604316,"user":{"displayName":"Sirya Caiazza","userId":"12380628864345214682"},"user_tz":-120},"id":"1L3tXYWjiuDp","outputId":"f917df96-d4a9-4321-a090-41ac8ef001ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing config.py\n"]}],"source":["# Write a file to retrieve the models\n","\n","%%writefile config.py\n","root_dir = '/content'\n","resnet_dir = root_dir + '/resnet_best_early_stop.pth'\n","custom_dir = root_dir + '/custom_best_early_stop.pth'"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":88,"status":"ok","timestamp":1752341604407,"user":{"displayName":"Sirya Caiazza","userId":"12380628864345214682"},"user_tz":-120},"id":"DyitdlHWEkod","outputId":"e0628811-5690-43be-837d-632cba7067c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing app.py\n"]}],"source":["%%writefile app.py\n","\n","# Import packages\n","import streamlit as st\n","import plotly.express as px\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import config\n","import torch.nn.functional as F\n","from torch.utils.tensorboard import SummaryWriter\n","from IPython.core.debugger import Tracer\n","import soundfile as sf\n","import torch.utils.data as data\n","from torchvision import transforms, utils, models, ops\n","from multiprocessing import cpu_count, Pool\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from PIL import Image\n","import seaborn as sns\n","import io\n","from tqdm import tqdm\n","import pandas as pd\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","import librosa\n","import librosa.display\n","import time\n","import librosa.effects\n","import IPython.display as ipd\n","from torch.utils.data import DataLoader, TensorDataset\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","import copy\n","\n","# Define the same functions of the code, but return the figures to display in the dashboard\n","def plot_waveplot(audio_data, sr):\n","    fig, ax = plt.subplots(figsize=(10, 3))\n","    ax.set_title('Waveplot of the audio', size=15)\n","    librosa.display.waveshow(audio_data, sr=sr, ax=ax)\n","    ax.set_xlabel('Time (s)')\n","    ax.set_ylabel('Amplitude')\n","    return fig\n","\n","def plot_spectrogram(audio_data, sr):\n","    X = librosa.stft(audio_data)\n","    Xdb = librosa.amplitude_to_db(abs(X))\n","    fig, ax = plt.subplots(figsize=(11, 3))\n","    ax.set_title('Spectrogram of the audio', size=15)\n","    img = librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz', ax=ax)\n","    fig.colorbar(img, ax=ax, format=\"%+2.f dB\")\n","    return fig\n","\n","def plot_mfcc(audio_data, sr):\n","    fig, ax = plt.subplots(figsize=(10, 4))\n","    mfcc_features = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=40)\n","    img = librosa.display.specshow(mfcc_features, x_axis='time', sr=sr, vmin=-1000, vmax=200, ax=ax)\n","    fig.colorbar(img, ax=ax, format='%+2.0f dB')\n","    ax.set_title('MFCC of the audio')\n","    return fig, mfcc_features\n","\n","# Create a class to preprocess the uploaded file audio\n","class InferenceDataset(data.Dataset):\n","    def __init__(self, ms=4000):\n","        self.ms = ms\n","        self.target_sr = 16000\n","        self.n_samples = self.target_sr * ms // 1000\n","\n","    def _preprocess(self, y, sr):\n","      try:\n","        # Convert to mono if needed\n","        if len(y.shape) > 1:\n","            y = librosa.to_mono(y)\n","        # Resample to 16000 Hz\n","        if sr != 16000:\n","            y = librosa.resample(y, orig_sr=sr, target_sr=self.target_sr)\n","        # Replicate if audio is too short\n","        if len(y) < self.n_samples:\n","            y = np.tile(y, self.n_samples // len(y) + 1)\n","        # Crop in the center\n","        start = (len(y) - self.n_samples) // 2\n","        y = y[start:start + self.n_samples]\n","\n","        # Define win_lenght and hop_lenght\n","        win_length = int(0.03 * self.target_sr)\n","        hop_length = int(0.015 * self.target_sr)\n","\n","        # Compute the MFCC\n","        mfcc = librosa.feature.mfcc(y=y, sr=self.target_sr, n_mfcc=40, win_length=win_length, hop_length=hop_length)\n","        # Normalize MFCC\n","        mfcc = (mfcc - np.mean(mfcc)) / np.std(mfcc)\n","        # Return the mfcc\n","        return mfcc\n","      except Exception as e:\n","        st.error(f\"Error during audio processing: {e}\")\n","        return None\n","\n","NUM_CLASSES = 14\n","NUM_MFCC_FEATURES = 40\n","NUM_TIME_FRAMES = 267\n","\n","# Copy the models to instantiate them in this code\n","# Define a custom AudioClassifier\n","class AudioClassifier(nn.Module):\n","  def __init__(self, num_mfcc_features = NUM_MFCC_FEATURES, num_time_frames = NUM_TIME_FRAMES, num_classes = NUM_CLASSES):\n","        super(AudioClassifier, self).__init__()\n","        self.relu = nn.ReLU() # Use ReLU as activiation\n","\n","        # Start by mapping the channel to 512 neurons and bottleneck it to the number of classes\n","\n","        self.conv0 = nn.Conv2d(1, 512, kernel_size=(5,5), stride=1, padding=2)\n","        self.bn0 = nn.BatchNorm2d(512) # Use batch normalization for stability\n","        self.pool = nn.MaxPool2d(kernel_size=(2,2), stride=2) # Use Maxpooling for efficiency\n","\n","        self.conv1 = nn.Conv2d(512, 512, kernel_size=(5,5), stride=1, padding=2)\n","        self.bn1 =  nn.BatchNorm2d(512)\n","\n","        self.conv2 = nn.Conv2d(512, 256, kernel_size=(5,5), stride=1, padding=2)\n","        self.bn2 = nn.BatchNorm2d(256)\n","\n","        self.conv3 = nn.Conv2d(256, 128, kernel_size=(5,5), stride=1, padding=2)\n","        self.bn3 =  nn.BatchNorm2d(128)\n","\n","        self.conv4 = nn.Conv2d(128, 64, kernel_size=(5,5), stride=1, padding=2)\n","        self.bn4 =  nn.BatchNorm2d(64)\n","\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1)) # Add temporal pooling\n","\n","        self.dropout1 = nn.Dropout(0.2) # Use some dropouts to prevent overfitting\n","        self.dropout2 = nn.Dropout(0.3)\n","        self.fc1 = nn.Linear(64, 32) # Add linear layers to combine features and determine class\n","        self.fc2 = nn.Linear(32, num_classes)\n","\n","  # Define the forward method\n","  def forward(self, x):\n","        x = self.pool(self.relu(self.bn0(self.conv0(x))))\n","        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n","        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n","        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n","        x = self.dropout1(x)\n","        x = self.pool(self.relu(self.bn4(self.conv4(x))))\n","\n","        x = self.avgpool(x)\n","        x = x.view(x.size(0), -1)\n","\n","        x = self.relu(self.fc1(x))\n","        x = self.dropout2(x)\n","        x = self.fc2(x)\n","        return x\n","\n","# Use ResNet18 for a pre-trained audio classifier\n","class PretrainedAudioClassifier(nn.Module):\n","    def __init__(self, num_mfcc_features = NUM_MFCC_FEATURES, num_time_frames = NUM_TIME_FRAMES, num_classes = NUM_CLASSES):\n","        super(PretrainedAudioClassifier, self).__init__()\n","        # Load the ResNet18 model\n","        self.resnet = models.resnet18(weights='DEFAULT')\n","        # Modify the first convolutional layer to accept 1 input channel\n","        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","        # Remove the original fully connected layer and substitute it with identity\n","        self.resnet.fc = nn.Identity()\n","        # Add adaptive average pooling 2D for temporal pooling\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        # Add classifier layer\n","        self.classifier = nn.Linear(512, num_classes) # 512 is the output feature size of ResNet18 before the FC layer\n","\n","    def forward(self, x):\n","        x = self.resnet(x)\n","        # Check if the dimension of the input are collapsed\n","        if x.dim() == 3:\n","            # Reshape to (batch_size, 1, 1, spatial_dim)\n","            x = x.unsqueeze(-2)\n","        elif x.dim() == 2:\n","             # Reshape to (batch_size, 1, 1, 1)\n","             x = x.unsqueeze(-1).unsqueeze(-1)\n","\n","        x = self.avgpool(x) # Apply adaptive average pooling 2D\n","        x = x.view(x.size(0), -1)  # Flatten the output\n","        x = self.classifier(x) # Output\n","        return x\n","\n","# --- Configuration ---\n","\n","# Set the page configuration for a wider layout\n","st.set_page_config(layout=\"wide\", page_title=\"Audio Analysis Dashboard\")\n","\n","# --- Page Header ---\n","st.title(\"ðŸ˜„ Emotion Classification Dashboard\")\n","st.markdown(\"\"\"\n","Welcome to the emotion classification dashboard! Upload your audio files in .wav format, visualize\n","their MFCC, waveplot and spectrogram,\n","and run inference with different models.\n","\"\"\")\n","\n","@st.cache_resource\n","def load_model(model_name):\n","    st.write(f\"Loading model: {model_name}...\")\n","    # Initialize the model to None\n","    model = None\n","    # Give the labels in the right order (alphabetical)\n","    labels = ['female_angry', 'female_disgust', 'female_fear', 'female_happy', 'female_neutral', 'female_sad', 'female_surprise', 'male_angry', 'male_disgust', 'male_fear', 'male_happy', 'male_neutral', 'male_sad', 'male_surprise']\n","    # Configure the hyperparameters\n","    NUM_CLASSES = len(labels)\n","    NUM_MFCC_FEATURES = 40\n","    NUM_TIME_FRAMES = 194\n","    try:\n","      if model_name == \"ResNet18\":\n","          # Initialize the model\n","          model = PretrainedAudioClassifier(NUM_MFCC_FEATURES, NUM_TIME_FRAMES, NUM_CLASSES)\n","          # Load the dictonary with all the information\n","          checkpoint = torch.load(config.resnet_dir, map_location=torch.device('cpu'))\n","          # Retrieve the state dictonary\n","          model_state_dict = checkpoint['net_state_dict']\n","          # Load the trained models\n","          model.load_state_dict(model_state_dict)\n","          # Return model and labels\n","          return model, labels\n","      # Do the same for the second model\n","      elif model_name == \"Custom Model\":\n","          model = AudioClassifier(NUM_MFCC_FEATURES, NUM_TIME_FRAMES, NUM_CLASSES)\n","          checkpoint = torch.load(config.custom_dir, map_location=torch.device('cpu'))\n","          model_state_dict = checkpoint['net_state_dict']\n","          model.load_state_dict(model_state_dict)\n","          return model, labels\n","      else:\n","          st.error(f\"Unknown model name: {model_name}\")\n","          return None, labels\n","      model.eval() # Set model to evaluation mode\n","      st.success(f\"{model_name} loaded successfully!\")\n","      return model, labels\n","    except Exception as e:\n","        st.error(f\"Error loading model '{model_name}': {e}. Check path, model architecture, and file integrity.\")\n","        return None, labels\n","\n","@st.cache_data\n","def preprocess_for_inference(audio_file_bytes):\n","    # Initialize everything to none\n","    y_raw, sr_raw = None, None\n","    processed_mfcc = None\n","\n","    try:\n","        # Load the audio file\n","        audio_io = io.BytesIO(audio_file_bytes)\n","\n","        # Read the wave and the sample rate\n","        y_raw, sr_raw = sf.read(audio_io)\n","\n","        # Ensure the type is compatible\n","        if y_raw.dtype != np.float32:\n","            y_raw = y_raw.astype(np.float32)\n","\n","        # Preprocess the file with the InferenceDataset class\n","        inference_processor = InferenceDataset(ms=4000)\n","        processed_mfcc = inference_processor._preprocess(y_raw, sr_raw)\n","\n","        return y_raw, sr_raw, processed_mfcc\n","\n","    except Exception as e:\n","        st.error(f\"Error during audio processing for inference: {e}\")\n","        st.error(f\"Type of uploaded file: {type(audio_file_bytes)}\")\n","        st.error(f\"Length of uploaded bytes: {len(audio_file_bytes)} bytes\")\n","        return None, None, None\n","\n","\n","st.header(\"ðŸ“¤ Upload Audio File\")\n","uploaded_file = st.file_uploader(\"Choose an audio file...\", type=[\"wav\"])\n","\n","#Initialize everyting to None\n","audio_data_raw = None\n","sr_raw = None\n","mfccs_for_model = None\n","\n","if uploaded_file is not None:\n","    # Read the file\n","    file_contents = uploaded_file.read()\n","    # Display the audio player directly from the uploaded file bytes\n","    st.audio(file_contents, format=f'audio/{uploaded_file.type.split(\"/\")[-1]}', start_time=0)\n","    st.success(\"File uploaded successfully!\")\n","\n","    with st.spinner(\"Processing audio and extracting features...\"):\n","        # Pass the byte stream to the processing function\n","        audio_data_raw, sr_raw, mfccs_for_model = preprocess_for_inference(file_contents)\n","\n","    if audio_data_raw is not None and mfccs_for_model is not None:\n","\n","        st.subheader(\"ðŸŽ§ Audio Features & Models Inference\")\n","        # Set two columns\n","        col1, col2 = st.columns([2, 1])  # Wider left column for plots\n","        with col1:\n","          st.subheader(\"ðŸ“Š Visualization of Audio Features\")\n","\n","          # Create tabs for better organization and plot the waveform, MFCC and spectrogram\n","          tab_waveform, tab_mfcc, tab_spectrogram = st.tabs([\"Waveform\", \"MFCC\", \"Spectrogram\"])\n","          with tab_waveform:\n","              st.write(\"#### Waveform\")\n","              fig_wave = plot_waveplot(audio_data_raw, sr_raw)\n","              st.pyplot(fig_wave)\n","              plt.close(fig_wave)\n","          with tab_mfcc:\n","                st.write(\"#### Mel-frequency Cepstral Coefficients (MFCCs)\")\n","                fig_mfcc, _ = plot_mfcc(audio_data_raw, sr_raw)\n","                st.pyplot(fig_mfcc)\n","                plt.close(fig_mfcc)\n","          with tab_spectrogram:\n","              st.write(\"#### Spectrogram\")\n","              fig_spec = plot_spectrogram(audio_data_raw, sr_raw)\n","              st.pyplot(fig_spec)\n","              plt.close(fig_spec)\n","        # In the second colum run the inferences\n","        with col2:\n","          st.header(\"ðŸ§  Model Inference\")\n","          model_options = [\"ResNet18\", \"Custom Model\"]\n","          selected_model_name = st.selectbox(\"Choose a model for inference:\", model_options)\n","          selected_model_idx = model_options.index(selected_model_name)\n","          current_model_name = model_options[selected_model_idx]\n","\n","          st.info(f\"Selected Model: **{current_model_name}**\")\n","\n","          if st.button(f\"Run Inference with {current_model_name}\"):\n","            if mfccs_for_model is not None:\n","                # Load the model\n","                model, labels = load_model(current_model_name)\n","\n","                if model and labels:\n","                    st.subheader(f\"Results from {current_model_name}:\")\n","                    with st.spinner(\"Running inference...\"):\n","                        try:\n","                            # Prepare MFCCs for the model, squeeze size to simulate batch size of 1 and add channel dimension\n","                            mfccs_tensor = torch.tensor(mfccs_for_model).float().unsqueeze(0).unsqueeze(0)\n","\n","                            with torch.no_grad(): # Disable gradient calculation for inference\n","                                outputs = model(mfccs_tensor)\n","                                # For classification transform all probabilities in order for them to be between [0,1]\n","                                probabilities = torch.softmax(outputs, dim=1)[0]\n","                                # Find the predicted class by finding the argmax and save its index using .item()\n","                                predicted_class_idx = torch.argmax(probabilities).item()\n","                                # Find the predicted label name\n","                                predicted_label = labels[predicted_class_idx]\n","\n","                                st.success(f\"**Prediction:** `{predicted_label}`\")\n","                                st.write(\"---\")\n","                                st.write(\"#### Probabilities:\")\n","                                # Display probabilities for all classes\n","                                prob_df = pd.DataFrame({\n","                                    'Class': labels,\n","                                    'Probability': probabilities.numpy()\n","                                })\n","                                prob_df = prob_df.sort_values(by='Probability', ascending=False)\n","                                st.dataframe(prob_df.set_index('Class'))\n","\n","                        except Exception as e:\n","                            st.error(f\"Error during model inference: {e}. Please check model input shape and loading.\")\n","                            st.write(f\"Expected MFCC shape: {mfccs_tensor.shape if 'mfccs_tensor' in locals() else 'N/A'}\")\n","                            st.write(f\"Raw MFCCs for model shape: {mfccs_for_model.shape}\")\n","                else:\n","                    st.error(\"Could not load the selected model. Please check model definitions and paths.\")\n","            else:\n","                st.warning(\"Please upload and preprocess an audio file first to run inference.\")\n","\n","else:\n","  st.info(\"Upload an audio file (.wav) to get started!\")"]},{"cell_type":"code","source":["# Run streamlit\n","!nohup streamlit run app.py &"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sgwC3-6-7HYV","executionInfo":{"status":"ok","timestamp":1752341604450,"user_tz":-120,"elapsed":47,"user":{"displayName":"Sirya Caiazza","userId":"12380628864345214682"}},"outputId":"2f643bec-9b1d-4b96-a85b-b4830432a7a9"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["nohup: appending output to 'nohup.out'\n"]}]},{"cell_type":"code","source":["# Use ngrok to run the app\n","from pyngrok import ngrok\n","!ngrok config add-authtoken 2zghj3h5IaUPeaXw810rFZiXotS_4zDtUxAVb65xq9m3LFHcb\n","# Setup a tunnel to the streamlit port 8501\n","public_url = ngrok.connect(addr=8501, proto=\"http\")\n","public_url"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d9eKj6XY7NAQ","executionInfo":{"status":"ok","timestamp":1752341607267,"user_tz":-120,"elapsed":2821,"user":{"displayName":"Sirya Caiazza","userId":"12380628864345214682"}},"outputId":"a8e66dc6-bbd6-4cd9-8af2-782b9cf997d8"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"]},{"output_type":"execute_result","data":{"text/plain":["<NgrokTunnel: \"https://b70c8d1ef150.ngrok-free.app\" -> \"http://localhost:8501\">"]},"metadata":{},"execution_count":5}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}