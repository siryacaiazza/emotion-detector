{"cells":[{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12268,"status":"ok","timestamp":1752164543720,"user":{"displayName":"Sirya Caiazza","userId":"08085234823560570975"},"user_tz":-120},"id":"gsDJW6eRBySe","outputId":"bcc54696-5c63-4612-ecaf-73285a7cc778"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.46.1)\n","Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n","Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n","Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n","Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n","Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n","Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n","Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n","Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n","Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n","Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n","Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n","Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n","Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n","Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n","Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n","Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n","Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n","Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.45.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.6.15)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.26.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n","Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.12)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n"]}],"source":["!pip install streamlit\n","\n","!pip install pyngrok"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1752164543764,"user":{"displayName":"Sirya Caiazza","userId":"08085234823560570975"},"user_tz":-120},"id":"1L3tXYWjiuDp","outputId":"4a0be56e-bee1-49f5-bc84-69b689155688"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting config.py\n"]}],"source":["%%writefile config.py\n","root_dir = '/content'\n","resnet_dir = root_dir + '/resnet_best_early_stop.pth'\n","custom_dir = root_dir + '/custom_best_early_stop.pth'"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":59,"status":"ok","timestamp":1752164543826,"user":{"displayName":"Sirya Caiazza","userId":"08085234823560570975"},"user_tz":-120},"id":"Ed04YCbQvgPj","outputId":"6431427c-9b51-4427-fb6d-168b62db705c"},"outputs":[{"output_type":"stream","name":"stdout","text":["app.py\t   custom_best_early_stop.pth  __pycache__\t\t   sample_data\n","config.py  nohup.out\t\t       resnet_best_early_stop.pth\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":98,"status":"ok","timestamp":1752164543927,"user":{"displayName":"Sirya Caiazza","userId":"08085234823560570975"},"user_tz":-120},"id":"DyitdlHWEkod","outputId":"9a35f934-d8b4-4e10-f4ff-3ff125757f36"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting app.py\n"]}],"source":["%%writefile app.py\n","\n","import streamlit as st\n","import plotly.express as px\n","# Import packages\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import config\n","import torch.nn.functional as F\n","from torch.utils.tensorboard import SummaryWriter\n","from IPython.core.debugger import Tracer\n","import soundfile as sf\n","import torch.utils.data as data\n","from torchvision import transforms, utils, models, ops\n","from multiprocessing import cpu_count, Pool\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from PIL import Image\n","import seaborn as sns\n","import io\n","from tqdm import tqdm\n","import pandas as pd\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","import librosa\n","import librosa.display\n","import time\n","import librosa.effects\n","import IPython.display as ipd\n","from torch.utils.data import DataLoader, TensorDataset\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","import copy\n","\n","def plot_waveplot(audio_data, sr):\n","    fig, ax = plt.subplots(figsize=(10, 3))\n","    ax.set_title('Waveplot of the audio', size=15)\n","    librosa.display.waveshow(audio_data, sr=sr, ax=ax)\n","    ax.set_xlabel('Time (s)')\n","    ax.set_ylabel('Amplitude')\n","    return fig\n","\n","def plot_spectrogram(audio_data, sr):\n","    X = librosa.stft(audio_data)\n","    Xdb = librosa.amplitude_to_db(abs(X))\n","    fig, ax = plt.subplots(figsize=(11, 3))\n","    ax.set_title('Spectrogram of the audio', size=15)\n","    img = librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz', ax=ax)\n","    fig.colorbar(img, ax=ax, format=\"%+2.f dB\")\n","    return fig\n","\n","def plot_mfcc(audio_data, sr):\n","    fig, ax = plt.subplots(figsize=(10, 4))\n","    mfcc_features = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=40)\n","    img = librosa.display.specshow(mfcc_features, x_axis='time', sr=sr, vmin=-1000, vmax=200, ax=ax)\n","    fig.colorbar(img, ax=ax, format='%+2.0f dB')\n","    ax.set_title('MFCC of the audio')\n","    return fig, mfcc_features\n","\n","class InferenceDataset(data.Dataset):\n","    def __init__(self, ms=4000):\n","        self.ms = ms\n","        self.target_sr = 16000\n","        self.n_samples = self.target_sr * ms // 1000\n","\n","\n","    def _preprocess(self, y, sr):\n","      try:\n","        # convert to mono if needed\n","        if len(y.shape) > 1:\n","            y = librosa.to_mono(y)\n","        # resample to 16000 Hz\n","        if sr != 16000:\n","            y = librosa.resample(y, orig_sr=sr, target_sr=self.target_sr)\n","        # replicate if audio is too short\n","        if len(y) < self.n_samples:\n","            y = np.tile(y, self.n_samples // len(y) + 1)\n","\n","        start = (len(y) - self.n_samples) // 2\n","        y = y[start:start + self.n_samples]\n","\n","\n","        win_length = int(0.03 * self.target_sr)\n","        hop_length = int(0.015 * self.target_sr)\n","\n","        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40, win_length=win_length, hop_length=hop_length)\n","        # normalize MFCCs\n","        mfcc = (mfcc - np.mean(mfcc)) / np.std(mfcc)\n","        # return features and label\n","        return mfcc\n","      except Exception as e:\n","        st.error(f\"Error during audio processing: {e}\")\n","        return None\n","\n","class AudioClassifier(nn.Module):\n","  def __init__(self, num_mfcc_features, num_time_frames, num_classes):\n","        super(AudioClassifier, self).__init__()\n","        self.relu = nn.ReLU()\n","\n","        self.conv0 = nn.Conv2d(1, 512, kernel_size=(5,5), stride=1, padding=2)\n","        self.bn0 = nn.BatchNorm2d(512)\n","        self.pool = nn.MaxPool2d(kernel_size=(2,2), stride=2)\n","\n","        self.conv1 = nn.Conv2d(512, 512, kernel_size=(5,5), stride=1, padding=2)\n","        self.bn1 =  nn.BatchNorm2d(512)\n","\n","        self.conv2 = nn.Conv2d(512, 256, kernel_size=(5,5), stride=1, padding=2)\n","        self.bn2 = nn.BatchNorm2d(256)\n","\n","        self.conv3 = nn.Conv2d(256, 128, kernel_size=(5,5), stride=1, padding=2)\n","        self.bn3 =  nn.BatchNorm2d(128)\n","\n","        self.conv4 = nn.Conv2d(128, 64, kernel_size=(5,5), stride=1, padding=2)\n","        self.bn4 =  nn.BatchNorm2d(64)\n","\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","\n","        self.dropout1 = nn.Dropout(0.2)\n","        self.dropout2 = nn.Dropout(0.3)\n","        self.fc1 = nn.Linear(64, 32)\n","        self.fc2 = nn.Linear(32, num_classes)\n","\n","  def forward(self, x):\n","        x = self.pool(self.relu(self.bn0(self.conv0(x))))\n","        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n","        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n","        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n","        x = self.dropout1(x)\n","        x = self.pool(self.relu(self.bn4(self.conv4(x))))\n","\n","        x = self.avgpool(x)\n","        x = x.view(x.size(0), -1)\n","\n","        x = self.relu(self.fc1(x))\n","        x = self.dropout2(x)\n","        x = self.fc2(x)\n","        return x\n","\n","class PretrainedAudioClassifier(nn.Module):\n","    def __init__(self, num_mfcc_features, num_time_frames, num_classes):\n","        super(PretrainedAudioClassifier, self).__init__()\n","        self.resnet = models.resnet18(weights='DEFAULT')\n","        # Modify the first convolutional layer to accept 1 input channel\n","        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","        # Remove the original fully connected layer\n","        self.resnet.fc = nn.Identity()\n","        # Add adaptive average pooling 2D\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","        # Classifier layer\n","        self.classifier = nn.Linear(512, num_classes) # 512 is the output feature size of ResNet18 before the FC layer\n","\n","    def forward(self, x):\n","        # Input x shape: (batch_size, 1, 40, 194)\n","        x = self.resnet(x)\n","        # After resnet, x should ideally be (batch_size, 512, H', W')\n","        # If the dimensions are collapsed, we need to ensure it's 4D for AdaptiveAvgPool2d\n","        # We can try to reshape it, assuming the last dimension is the flattened spatial one if it's 3D\n","        if x.dim() == 3:\n","            # Assuming shape is (batch_size, channels, spatial_dim), reshape to (batch_size, channels, 1, spatial_dim)\n","            x = x.unsqueeze(-2)\n","        elif x.dim() == 2:\n","             # This case is less likely but handle it by reshaping to (batch_size, channels, 1, 1)\n","             x = x.unsqueeze(-1).unsqueeze(-1)\n","\n","\n","        x = self.avgpool(x) # Apply adaptive average pooling 2D, output shape (batch_size, 512, 1, 1)\n","        x = x.view(x.size(0), -1)  # Flatten the output to (batch_size, 512)\n","        x = self.classifier(x) # Output shape (batch_size, num_classes)\n","        return x\n","\n","# --- Configuration ---\n","# Set the page configuration for a wider layout\n","st.set_page_config(layout=\"wide\", page_title=\"Audio Analysis Dashboard\")\n","\n","# --- Page Header ---\n","st.title(\"🎶 Emotion Detection Dashboard\")\n","st.markdown(\"\"\"\n","Welcome to the audio analysis dashboard! Upload your audio files, preprocess them,\n","and run inference with different models.\n","\"\"\")\n","\n","@st.cache_resource\n","def load_model(model_name):\n","    # This is where you would load your actual .pth, .h5, or .pkl files\n","    # For demonstration, we'll create dummy models\n","    st.write(f\"Loading model: {model_name}...\")\n","    model = None\n","    labels = ['female_angry', 'female_disgust', 'female_fear', 'female_happy', 'female_neutral', 'female_sad', 'female_surprise', 'male_angry', 'male_disgust', 'male_fear', 'male_happy', 'male_neutral', 'male_sad', 'male_surprise']\n","    NUM_CLASSES = len(labels)\n","    NUM_MFCC_FEATURES = 40\n","    NUM_TIME_FRAMES = 194\n","    try:\n","      if model_name == \"ResNet18\":\n","          # Load your actual Model\n","          model = PretrainedAudioClassifier(NUM_MFCC_FEATURES, NUM_TIME_FRAMES, NUM_CLASSES)\n","          checkpoint = torch.load(config.resnet_dir, map_location=torch.device('cpu'))\n","          model_state_dict = checkpoint['net_state_dict']\n","          model.load_state_dict(model_state_dict)\n","          return model, labels\n","\n","      elif model_name == \"Custom Model\":\n","          model = AudioClassifier(NUM_MFCC_FEATURES, NUM_TIME_FRAMES, NUM_CLASSES)\n","          checkpoint = torch.load(config.custom_dir, map_location=torch.device('cpu'))\n","          model_state_dict = checkpoint['net_state_dict']\n","          model.load_state_dict(model_state_dict)\n","          return model, labels\n","      else:\n","          st.error(f\"Unknown model name: {model_name}\")\n","          return None, labels\n","      model.eval() # Set model to evaluation mode\n","      st.success(f\"{model_name} loaded successfully!\")\n","      return model, labels\n","    except Exception as e:\n","        st.error(f\"Error loading model '{model_name}': {e}. Check path, model architecture, and file integrity.\")\n","        return None, labels\n","\n","@st.cache_data\n","def preprocess_for_inference(audio_file_bytes):\n","    y_raw, sr_raw = None, None\n","    processed_mfcc = None\n","\n","    try:\n","        # Use soundfile directly with BytesIO as librosa.load can have issues with it for some formats/setups\n","        # sf.read expects the file-like object to be seekable and readable\n","        audio_io = io.BytesIO(audio_file_bytes)\n","\n","        # This is the line that caused 'sf' not defined before, ensure 'import soundfile as sf' is present\n","        y_raw, sr_raw = sf.read(audio_io)\n","\n","        # sf.read returns float64, librosa features sometimes prefer float32, and always mono for MFCC.\n","        if y_raw.dtype != np.float32:\n","            y_raw = y_raw.astype(np.float32)\n","\n","        # Ensure audio is mono, as required by librosa.to_mono\n","        if y_raw.ndim > 1: # Check if it has multiple channels\n","            y_raw = librosa.to_mono(y_raw.T) # .T transposes for channels-last format if needed\n","\n","        inference_processor = InferenceDataset(ms=4000)\n","        processed_mfcc = inference_processor._preprocess(y_raw, sr_raw)\n","\n","        return y_raw, sr_raw, processed_mfcc\n","\n","    except Exception as e:\n","        st.error(f\"Error during audio processing for inference: {e}\")\n","        st.error(f\"Type of uploaded file: {type(audio_file_bytes)}\")\n","        st.error(f\"Length of uploaded bytes: {len(audio_file_bytes)} bytes\")\n","        return None, None, None\n","\n","\n","st.header(\"📤 Upload Audio File\")\n","uploaded_file = st.file_uploader(\"Choose an audio file...\", type=[\"wav\"])\n","\n","audio_data_raw = None\n","sr_raw = None\n","mfccs_for_model = None\n","\n","if uploaded_file is not None:\n","\n","    file_contents = uploaded_file.read()\n","    # Display the audio player directly from the uploaded file bytes\n","    st.audio(file_contents, format=f'audio/{uploaded_file.type.split(\"/\")[-1]}', start_time=0)\n","    st.success(\"File uploaded successfully!\")\n","\n","    with st.spinner(\"Processing audio and extracting features...\"):\n","        # Pass the byte stream to our processing function\n","        audio_data_raw, sr_raw, mfccs_for_model = preprocess_for_inference(file_contents)\n","\n","    if audio_data_raw is not None and mfccs_for_model is not None:\n","        st.subheader(\"📊 Audio Features & Visualizations\")\n","\n","        # Create tabs for better organization\n","        tab_waveform, tab_mfcc, tab_spectrogram = st.tabs([\"Waveform\", \"MFCC\", \"Spectrogram\"])\n","        with tab_waveform:\n","            st.write(\"#### Waveform\")\n","            fig_wave = plot_waveplot(audio_data_raw, sr_raw)\n","            st.pyplot(fig_wave)\n","            plt.close(fig_wave)\n","        with tab_mfcc:\n","              st.write(\"#### Mel-frequency Cepstral Coefficients (MFCCs)\")\n","              # Note: plot_mfcc is called with raw audio data to recompute for display purposes\n","              # mfccs_for_model variable holds the *preprocessed* mfccs for the model\n","              fig_mfcc, _ = plot_mfcc(audio_data_raw, sr_raw)\n","              st.pyplot(fig_mfcc)\n","              plt.close(fig_mfcc)\n","        with tab_spectrogram:\n","            st.write(\"#### Spectrogram\")\n","            fig_spec = plot_spectrogram(audio_data_raw, sr_raw)\n","            st.pyplot(fig_spec)\n","            plt.close(fig_spec)\n","        st.header(\"🧠 Model Inference\")\n","        model_options = [\"ResNet18\", \"Custom Model\"]\n","        selected_model_name = st.selectbox(\"Choose a model for inference:\", model_options)\n","        selected_model_idx = model_options.index(selected_model_name)\n","        current_model_name = model_options[selected_model_idx]\n","\n","        st.info(f\"Selected Model: **{current_model_name}**\")\n","\n","        if st.button(f\"Run Inference with {current_model_name}\"):\n","          if mfccs_for_model is not None:\n","              model, labels = load_model(current_model_name)\n","\n","              if model and labels:\n","                  st.subheader(f\"Results from {current_model_name}:\")\n","                  with st.spinner(\"Running inference...\"):\n","                      try:\n","                          # Prepare MFCCs for the model\n","                          # Assuming your PyTorch model expects (batch_size, 1, n_mfcc, n_frames)\n","                          # You might need to adjust this reshaping based on your actual model's input\n","                          mfccs_tensor = torch.tensor(mfccs_for_model).float().unsqueeze(0).unsqueeze(0)\n","\n","                          with torch.no_grad(): # Disable gradient calculation for inference\n","                              outputs = model(mfccs_tensor)\n","                              # For classification, typically use argmax or softmax\n","                              probabilities = torch.softmax(outputs, dim=1)[0]\n","                              predicted_class_idx = torch.argmax(probabilities).item()\n","                              predicted_label = labels[predicted_class_idx]\n","\n","                              st.success(f\"**Prediction:** `{predicted_label}`\")\n","                              st.write(\"---\")\n","                              st.write(\"#### Probabilities:\")\n","                              # Display probabilities for all classes\n","                              prob_df = pd.DataFrame({\n","                                  'Class': labels,\n","                                  'Probability': probabilities.numpy()\n","                              })\n","                              prob_df = prob_df.sort_values(by='Probability', ascending=False)\n","                              st.dataframe(prob_df.set_index('Class'))\n","\n","                      except Exception as e:\n","                          st.error(f\"Error during model inference: {e}. Please check model input shape and loading.\")\n","                          st.write(f\"Expected MFCC shape: {mfccs_tensor.shape if 'mfccs_tensor' in locals() else 'N/A'}\")\n","                          st.write(f\"Raw MFCCs for model shape: {mfccs_for_model.shape}\")\n","              else:\n","                  st.error(\"Could not load the selected model. Please check model definitions and paths.\")\n","          else:\n","              st.warning(\"Please upload and preprocess an audio file first to run inference.\")\n","\n","else:\n","  st.info(\"Upload an audio file (.wav) to get started!\")\n"]},{"cell_type":"code","source":["!nohup streamlit run app.py &"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sgwC3-6-7HYV","executionInfo":{"status":"ok","timestamp":1752164544023,"user_tz":-120,"elapsed":92,"user":{"displayName":"Sirya Caiazza","userId":"08085234823560570975"}},"outputId":"7bfbb876-4031-498e-83bc-5672388aa17f"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["nohup: appending output to 'nohup.out'\n"]}]},{"cell_type":"code","source":["from pyngrok import ngrok\n","!ngrok config add-authtoken 2zghj3h5IaUPeaXw810rFZiXotS_4zDtUxAVb65xq9m3LFHcb\n","# Setup a tunnel to the streamlit port 8501\n","public_url = ngrok.connect(addr=8501, proto=\"http\")\n","public_url"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d9eKj6XY7NAQ","executionInfo":{"status":"ok","timestamp":1752164544944,"user_tz":-120,"elapsed":926,"user":{"displayName":"Sirya Caiazza","userId":"08085234823560570975"}},"outputId":"c901c5bb-dcdd-4774-8b8c-a12241874361"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"]},{"output_type":"execute_result","data":{"text/plain":["<NgrokTunnel: \"https://02a3543e7ceb.ngrok-free.app\" -> \"http://localhost:8501\">"]},"metadata":{},"execution_count":12}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}